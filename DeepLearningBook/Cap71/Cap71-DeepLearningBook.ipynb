{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capítulo 71 - Projeto Inteligência Artificial Para Jogar Blackjack (Em Python e PyTorch)\n",
    "\n",
    "## Parte 1 - Testando o Ambiente com Política Randômica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este Jupyter Notebook é parte integrante do Deep Learning Book:\n",
    "    \n",
    "www.deeplearningbook.com.br   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"685\" alt=\"imagem1\" src=\"https://user-images.githubusercontent.com/18689183/83305195-216a7980-a1b5-11ea-97b4-4840c1a8b174.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta aula considera que você possui conhecimento em Linguagem Python. Se esse não for o caso, acesse nosso curso gratuito e comece aprender agora mesmo: <a href=\"https://www.datascienceacademy.com.br/course?courseid=python-fundamentos\">Python Fundamentos Para Análise de Dados</a>.\n",
    "\n",
    "Consideramos ainda que você já estudou os capítulos anteriores do Deep Learning Book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eu sei que é difícil acreditar, então antes que você pergunte, já vou responder: Sim, o Projeto é inteiramente gratuito, sem custo, sem pegadinha. Ao final você terá um Assistente Virtual de IA Para Jogar Blackjack.\n",
    "\n",
    "Nosso objetivo é ajudar a desenvolver e democratizar o conhecimento de Inteligência Artificial no Brasil.\n",
    "\n",
    "Precisaremos de diversas aulas para construir o Assistente Virtual, treinar, testar e avaliar e durante as aulas vamos explorar outros temas ligados ao Aprendizado Por Reforço que estudamos nos capítulos anteriores. Não fique ansioso e aproveite a jornada, pois haverá muito conhecimento e aprendizado durante todas as aulas.\n",
    "\n",
    "Um projeto como esse tem nível avançado, mas faremos o nosso melhor para explicar tudo bem passo a passo. Vamos começar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição do Problema\n",
    "\n",
    "Conforme ensinamos em <a href=\"https://www.datascienceacademy.com.br/pages/todos-os-cursos-dsa\">todos os cursos da DSA</a>, qualquer projeto de Ciência de Dados, Machine Learning ou IA deve começar com a definição clara do problema a ser resolvido, para que então possamos escolher as melhores ferramentas, técnicas, procedimentos e métodos de avaliação. Como diz aquele velho ditado: \"Se você não sabe para onde vai, qualquer caminho serve\". Para escolher o caminho ideal, primeiro temos que definir para onde estamos indo.\n",
    "\n",
    "### Jogando Blackjack\n",
    "\n",
    "Você conhece Blackjack? Não? Então terá que aprender um pouco sobre o jogo, afinal teremos que modelar as regras e treinar nosso Agente Inteligente. Explicarei tudo em detalhes aqui para você, no já conhecido padrão de qualidade DSA.\n",
    "\n",
    "O jogo de cartas Blackjack, também conhecido como 21, é um dos jogos mais conhecidos do mundo. \n",
    "\n",
    "Blackjack (ou 21) é um jogo praticado com cartas e que pode ser jogado com 1 a 8 baralhos de 52 cartas, em que o objetivo é ter mais pontos do que o adversário, mas sem ultrapassar os 21 (caso em que se perde). \n",
    "\n",
    "A mão mais elevada no Blackjack é um Ás e uma carta de 10 pontos e é chamada justamente de Blackjack. Se o jogador e o dealer (a banca ou cassino) tiverem um Blackjack a aposta é um empate. O jogador ganha se a sua mão tiver mais pontos que a do dealer, sem ir acima de 21. Assim, uma mão de 21 pontos é a mais elevada e é por isso que o jogo é chamado às vezes de 21. Se o jogador ou o dealer forem acima de 21 perdem automaticamente.\n",
    "\n",
    "O jogo ficou ainda mais famoso depois do filme chamado: 21\n",
    "\n",
    "https://www.imdb.com/title/tt0478087/\n",
    "\n",
    "Blackjack é um problema de negócio como qualquer outro, porém um pouco mais divertido, eu diria! Vamos construir um Agente de Inteligência Artificial que vai aprender a jogar Blackjack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**O conteúdo do Projeto pode ser usado livremente e pedimos apenas a gentileza de citar a DSA.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ferramentas\n",
    "\n",
    "Usaremos Linguagem Python para a programação e o PyTorch como nosso famework de Deep Learning para a construção das políticas e treinamento do agente. Aqui está o site oficial o PyTorch:\n",
    "\n",
    "https://pytorch.org/\n",
    "\n",
    "Caso não tenha familiaridade com o PyTorch você pode aprender clicando <a href=\"https://www.datascienceacademy.com.br/course?courseid=deep-learning-frameworks\">aqui</a>.\n",
    "\n",
    "Na construção do modelo usaremos Apendizado Por Reforço como nossa estratégia de Machine Learning, criando uma Deep Q-Network que estudamos nos capítulos anteriores aqui do Deep Learning Book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambiente\n",
    "\n",
    "Mas desenvolver um ambiente para o jogo a partir do zero para treinar o agente seria extremamente trabalhoso, concorda? Teríamos que praticamente desenvolver um jogo de Blackjack no computador. É aqui onde o Gym vai mostrar seu valor, mais uma vez.\n",
    "\n",
    "A equipe do Gym oferece ambientes prontos que você carrega em seu código e usa para treinar seu agente. Assim, podemos concentrar nossos esforços no algoritmo do agente, sem ter a preocupação inicial com o desenvolvimento do ambiente (para aplicações comerciais, teremos que desenvolver o ambiente também).\n",
    "\n",
    "Aqui você encontra o código fonte do ambiente:\n",
    "\n",
    "https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição do Ambiente\n",
    "\n",
    "Vamos descrever o jogo, pois cada uma das etapas e regras terá que ser modelada em nosso projeto, para que o Agente Inteligente consiga aprender.\n",
    "\n",
    "O Blackjack é um jogo de cartas em que o objetivo é obter cartas que totalizem o mais próximo possível dos 21 sem ir além. Em um cassino, por exemplo, você seria o jogador e o seu oponente seria o dealer.\n",
    "\n",
    "As cartas Valete, Dama e Rei têm valor de ponto 10. Os Ases podem contar como 11 ou 1, e é chamado de 'utilizável' aos 11. Este jogo é colocado com um baralho infinito (ou com substituição). O jogo começa com cada um (jogador e dealer) tendo uma carta com a face para cima e outra com a face para baixo.\n",
    "\n",
    "O jogador pode solicitar cartas adicionais (Hit = 1) até que eles decidam parar (Stick = 0) ou exceda 21 (Bust). Depois que o jogador entra, o carteador revela sua carta virada para baixo e compra até que sua soma seja 17 ou mais. Se o croupier falir, o jogador ganha. Se nem o jogador nem o dealter podem seguir adiante, o resultado (ganhar, perder, empatar) é decidido pela soma mais próxima de 21. A recompensa pela vitória é +1, o empate é 0 e a derrota é -1. Um resumo aqui:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"464\" alt=\"imagem2\" src=\"https://user-images.githubusercontent.com/18689183/83305320-62fb2480-a1b5-11ea-9293-0221598b8c9a.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iniciando o Desenvolvimento\n",
    "\n",
    "Nesta Parte 1 vamos testar o ambiente com uma política randômica e nas próximas partes trabalharemos no treinamento e políticas de aprendizado do agente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para executar no Google Colab, clique no menu superior em Runtime - Run all\n",
    "\n",
    "No mesmo menu, em Change Runtime Type, você pode alterar a execução para GPU ou TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão da Linguagem Python Neste Ambiente de Desenvolvimento: 3.7.6\n"
     ]
    }
   ],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Neste Ambiente de Desenvolvimento:', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pacote para criar marca d'agua com as versões dos pacotes\n",
    "!pip install -q watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala o pacote gym\n",
    "!pip install -q gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala o pacote torch (PyTorch)\n",
    "!pip install -q torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6kVd1ovCggTd"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys \n",
    "import pdb\n",
    "import gym\n",
    "import time \n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.envs.toy_text import blackjack\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch      1.5.0\n",
      "numpy      1.18.4\n",
      "gym        0.10.11\n",
      "matplotlib 3.2.1\n",
      "Data Science Academy\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando o Ambiente\n",
    "\n",
    "Vamos testar o ambiente e verificar se está respondendo de forma correta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10, False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cria uma instância de um ambiente que facilita o desenvolvimento e teste do nosso agente\n",
    "env = gym.make('Blackjack-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o ambiente criado, verificamos quais ações estão disponíveis para o agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espaço de Ação: Discrete(2). \n",
      "\n",
      "=> (1 = Hit, 0 = Bust)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extraímos env.action_space para verificar o espaço de ações disponíveis para o agente\n",
    "print(f'Espaço de Ação: {env.action_space}. \\n\\n=> (1 = Hit, 0 = Bust)\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso agente tem duas ações possíveis no ambiente: solicitar mais uma carta (Hit) ou desistir e perder (Bust).\n",
    "    \n",
    "Vamos checar o espaço de observação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espaço de Observação: Tuple(Discrete(32), Discrete(11), Discrete(2)) \n",
      "\n",
      "=> (Total da Mão do Jogador, Carta do Dealer, Ás Utilizável)\n"
     ]
    }
   ],
   "source": [
    "# Extraímos env.observation_space para verificar o espaço de observação disponível para o agente\n",
    "print(f'Espaço de Observação: {env.observation_space} \\n\\n=> (Total da Mão do Jogador, Carta do Dealer, Ás Utilizável)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que o resultado é uma tupla de valores discretos.\n",
    "\n",
    "Vamos executar um passo no ambiente (ou seja, efetuar uma jogada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espaço de Observação:\n",
      "=>  ((5, 10, False), -1.0, True, {})\n"
     ]
    }
   ],
   "source": [
    "print(\"Espaço de Observação:\")\n",
    "print('=> ', env.step(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso ambiente está funcionando! Agora o desafio é fazer o Agente aprender sozinho, jogando muitos e muitos episódios de Blackjack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos começar criando um pequeno algoritmo de uma política randômica, apenas para testar o ambiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando Uma Política Randômica\n",
    "\n",
    "Durante a construção do processo de treinamento do agente, iremos definir regras de recompensas e penalidades, por acertos e erros respectivamente. Mas antes, vamos checar qual seria o resultado com uma política randômica, ou seja, sem nenhum critério de aprendizado definido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado:\n",
      "(13, 10, False)\n",
      "\n",
      "O Jogo Terminou! Sua Recompensa:  1.0\n",
      "Você Venceu :)\n",
      "\n",
      "Estado:\n",
      "(13, 6, False)\n",
      "Estado:\n",
      "(19, 6, False)\n",
      "Estado:\n",
      "(21, 6, False)\n",
      "\n",
      "O Jogo Terminou! Sua Recompensa:  1.0\n",
      "Você Venceu :)\n",
      "\n",
      "Estado:\n",
      "(17, 8, False)\n",
      "\n",
      "O Jogo Terminou! Sua Recompensa:  -1\n",
      "Você Perdeu :(\n",
      "\n",
      "Estado:\n",
      "(9, 1, False)\n",
      "Estado:\n",
      "(13, 1, False)\n",
      "\n",
      "O Jogo Terminou! Sua Recompensa:  -1\n",
      "Você Perdeu :(\n",
      "\n",
      "Estado:\n",
      "(15, 6, False)\n",
      "\n",
      "O Jogo Terminou! Sua Recompensa:  -1\n",
      "Você Perdeu :(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop  de 5 episódios do jogo para a política randômica\n",
    "for episode in range(5):\n",
    "    \n",
    "    # Reset do ambiente\n",
    "    estado = env.reset()\n",
    "    \n",
    "    # Enquanto for verdadeiro (enquanto o agente estiver jogando)\n",
    "    while True:\n",
    "        \n",
    "        # Imprime o estado\n",
    "        print('Estado:')\n",
    "        print(estado)\n",
    "        \n",
    "        # Toma uma ação de forma aleatória a partir do espaço de ações disponíveis no ambiente\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        # Coletamos o resultado de um passo (ou uma jogada) \n",
    "        # Temos o estado, a recompensa, se o jogo terminou e informações\n",
    "        estado, recompensa, done, info = env.step(action)\n",
    "        \n",
    "        # Se o jogo terminou\n",
    "        if done:\n",
    "            \n",
    "            # Imprime a recompensa do agente\n",
    "            print('\\nO Jogo Terminou! Sua Recompensa: ', recompensa)\n",
    "            \n",
    "            # Imprime se agente venceu ou perdeu\n",
    "            print('Você Venceu :)\\n') if recompensa > 0 else print('Você Perdeu :(\\n')\n",
    "            \n",
    "            # Encerra o loop neste episódio\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O agente consegue chegar a um resultado satisfatório, alternando entre vitórias e derrotas, com uma política randômica. Nosso trabalho é criar um processo de aprendizado que seja melhor que uma política randômica. Aliás, isso vale para qualquer coisa em Machine Learning e por esse motivo Cientistas de Dados são profissionais tão cobiçados no mercado de trabalho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando Uma Política Randômica \"Mais Pesada\"\n",
    "\n",
    "O exemplo anterior considerou apenas 5 episódios. Mas o que aconteceria com 1000 episódios? Cada espisódio é um jogo completo, do início até vencer, empatar ou perder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo as variáveis de controle\n",
    "\n",
    "# Número de rounds\n",
    "num_rounds = 1000 \n",
    "\n",
    "# Números de amostras\n",
    "num_samples = 100 \n",
    "\n",
    "# Lista para armazenar a recompensa média\n",
    "average_payouts = []\n",
    "\n",
    "# Gravamos o horário que começou o treinamento\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop pelo range de amostras\n",
    "for sample in range(num_samples):\n",
    "    \n",
    "    # Inicialia o round\n",
    "    round = 1\n",
    "    \n",
    "    # Recompensa total\n",
    "    total_payout = 0 \n",
    "    \n",
    "    # Loop pelo número de rounds\n",
    "    while round <= num_rounds:\n",
    "        \n",
    "        # Toma ação aleatória\n",
    "        action = env.action_space.sample()  \n",
    "        \n",
    "        # Extrai o resultado de cada passada (cada jogada)\n",
    "        # Como não queremos info, colocamos apenas _\n",
    "        obs, payout, is_done, _ = env.step(action)\n",
    "        \n",
    "        # Totaliza a recompensa\n",
    "        total_payout += payout\n",
    "        \n",
    "        # Verifica se o jogo terminou\n",
    "        if is_done:\n",
    "            \n",
    "            # Reset do ambiente\n",
    "            # O ambiente negocia novas cartas para o jogador e o dealer\n",
    "            env.reset() \n",
    "            \n",
    "            # Totaliza o round\n",
    "            round += 1\n",
    "    \n",
    "    # Armazena as recompensas\n",
    "    average_payouts.append(total_payout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recompensa média depois de 1000 rounds = -399.9\n",
      "Recompensa média por round = -0.400\n"
     ]
    }
   ],
   "source": [
    "# Vamos imprimir o resultado\n",
    "print(\"\\nRecompensa média depois de {} rounds = {}\".format(num_rounds, sum(average_payouts)/num_samples))\n",
    "print(f\"Recompensa média por round = {sum(average_payouts) / num_samples / num_rounds:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute as 2 células anteriores várias vezes. Percebeu o que aconteceu? Com  muitos episódios jogados, o agente tem um desempenho ruim, pois ele não está aprendendo, apenas jogando de forma aleatória. É por isso que precisamos definir um processo de aprendizado. \n",
    "\n",
    "É o que faremos no próximo capítulo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:**\n",
    "\n",
    "Gostaríamos de agradecer todo o feedback que temos recebido sobre o Deep Learning Book. São vários e-mails de pessoas parabenizando pelo trabalho, trazendo sugestões, apontando possíveis melhorias e solicitando autorização para citação em trabalhos de conclusão de curso ou mesmo teses acadêmicas. Muitos professores de cursos de graduação e pós-graduação recomendam este livro para seus alunos.\n",
    "\n",
    "Focamos no trabalho árduo diário, pois acreditamos que conhecimento transforma vidas e é disso que o Brasil precisa. Siga firme e seja um elemento de transformação positiva. \n",
    "\n",
    "Trabalhe bastante, até que seu trabalho não possa mais ser ignorado!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estude, aprenda e divirta-se! \n",
    "\n",
    "Algumas aplicações comerciais de IA de última geração usam as técnicas que você encontra no trabalho deste e dos próximos capítulos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acompanhe as publicações diárias em nossas redes sociais:\n",
    "\n",
    "<a href=\"https://www.linkedin.com/company/data-science-academy/\">LinkedIn</a>\n",
    "\n",
    "<a href=\"https://www.instagram.com/dsacademybr/\">Instagram</a>\n",
    "\n",
    "<a href=\"https://twitter.com/dsacademybr\">Twitter</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/dsacademybr\">Facebook</a>\n",
    "\n",
    "<a href=\"https://www.youtube.com/channel/UCyoi9yYaQRQ2F34wDLosr2A?reload=9\">Youtube</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscando dicas de carreira e tendências? Então confira nosso Blog:\n",
    "    \n",
    "<a href=\"http://datascienceacademy.com.br/blog\">Blog DSA</a>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui você encontra todos os Cursos e Formações DSA:\n",
    "\n",
    "<a href=\"https://www.datascienceacademy.com.br/pages/todos-os-cursos-dsa\">Catálogo de Cursos</a>\n",
    "\n",
    "<a href=\"https://www.datascienceacademy.com.br/pages/cursos-gratuitos-1\">Curso Gratuitos</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "qlearn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
